import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score, mean_absolute_error
from datetime import datetime, timedelta
from vnstock import Vnstock
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.layers import Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping

def train_with_XGBoost(ticker, num_days=350, test_size=0.2):
    """
    Train m√¥ h√¨nh XGBoost v·ªõi s·ªë ng√†y d·ªØ li·ªáu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    
    Args:
        ticker (str): M√£ c·ªï phi·∫øu
        num_days (int): S·ªë ng√†y d·ªØ li·ªáu mu·ªën s·ª≠ d·ª•ng
        test_size (float): T·ª∑ l·ªá d·ªØ li·ªáu test (0-1)
        
    Returns:
        dict: K·∫øt qu·∫£ train v√† ƒë√°nh gi√° m√¥ h√¨nh
    """
    try:
        end_date = datetime.now()
        end_date_str = end_date.strftime('%Y-%m-%d')
        
        stock = Vnstock().symbol(symbol=ticker)
        df =stock.quote.history(start = '2000-01-01', end= end_date_str, interval='1D')


        numeric_columns = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_columns:
            if col in df.columns:
                if df[col].dtype == 'object':
                    try:
                        df[col] = df[col].str.replace(',', '').astype(float)
                    except Exception as e:
                        return None
        
        df['volatility'] = df['high'] - df['low']
        df['return_1d'] = df['close'].pct_change()
        df['ma5'] = df['close'].rolling(window=5).mean()
        df['ma10'] = df['close'].rolling(window=10).mean()
        df['volume_ma5'] = df['volume'].rolling(window=5).mean()
        
        df['volume_ratio'] = np.where(
            df['volume_ma5'] > 0, 
            df['volume'] / df['volume_ma5'], 
            np.nan
        )

        df['target'] = df['close'].shift(-1)
        
        df = df.replace([np.inf, -np.inf], np.nan)
        df.dropna(inplace=True)

        total_days = len(df)
        if total_days < num_days:
            # N·∫øu mu·ªën ti·∫øp t·ª•c v·ªõi s·ªë ng√†y hi·ªán c√≥, gi·ªØ nguy√™n d√≤ng d∆∞·ªõi
            num_days = total_days
            # N·∫øu mu·ªën b·ªè qua ticker kh√¥ng ƒë·ªß d·ªØ li·ªáu, uncomment d√≤ng d∆∞·ªõi
            # return None
        
        df = df.tail(num_days).reset_index(drop=True)
        
        features = ['open', 'high', 'low', 'close', 'volume', 'volatility', 
                   'return_1d', 'ma5', 'ma10', 'volume_ratio']
        
        X = df[features]
        y = df['target']
        
        test_rows = int(len(df) * test_size)
        train_rows = len(df) - test_rows
        
        X_train, X_test = X.iloc[:train_rows], X.iloc[train_rows:]
        y_train, y_test = y.iloc[:train_rows], y.iloc[train_rows:]
        
        
        model = xgb.XGBRegressor(n_estimators=300, max_depth=2, learning_rate=0.05,
                                 colsample_bytree=0.8, gamma=0.1, min_child_weight=3, subsample=0.8)
        model.fit(X_train, y_train)
        
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        

        y_direction_true = (y_test.values > X_test['close'].values).astype(int)
        y_direction_pred = (y_pred > X_test['close'].values).astype(int)
        direction_accuracy = np.mean(y_direction_true == y_direction_pred)
        

        latest = df.iloc[-1:].copy()
        X_future = latest[features]
        predicted_price_tomorrow = model.predict(X_future)[0]
        current_price = df['close'].iloc[-1]
        change_pct = (predicted_price_tomorrow - current_price) / current_price * 100
        
        if change_pct > 0:
            trend = "üîº TƒÇNG"
        else:
            trend = "üîΩ GI·∫¢M"

        return {
            'ticker': ticker,
            'data_days': len(df),
            'train_days': len(X_train),
            'test_days': len(X_test),
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'direction_accuracy': direction_accuracy,
            'current_price': current_price,
            'predicted_price': predicted_price_tomorrow,
            'change_pct': change_pct,
            'trend': 'UP' if change_pct > 0 else 'DOWN'
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None
    
def train_LSTM(ticker):
    """
    Train m√¥ h√¨nh LTSM
    
    Args:
        ticker (str): M√£ c·ªï phi·∫øu
    Returns:
        dict: K·∫øt qu·∫£ train v√† ƒë√°nh gi√° m√¥ h√¨nh
    """
    try:
        end_date = datetime.now()
        end_date_str = end_date.strftime('%Y-%m-%d')
        
        stock = Vnstock().symbol(symbol=ticker)
        df =stock.quote.history(start = '2000-01-01', end= end_date_str, interval='1D')

        if 'open' not in df.columns:
            features = ['close', 'volume', 'high', 'low']
        else:
            features = ['close', 'open', 'volume', 'high', 'low']
        

        df_features = df.copy()
        
        if (df_features['volume'] <= 0).any():
            min_positive_volume = df_features.loc[df_features['volume'] > 0, 'volume'].min()
            df_features.loc[df_features['volume'] <= 0, 'volume'] = min_positive_volume * 0.01
            
        for price_col in ['close', 'high', 'low']:
            if (df_features[price_col] <= 0).any():
                return f"D·ªØ li·ªáu gi√° b·ªã l·ªói, kh√¥ng th·ªÉ ti·∫øn h√†nh d·ª± ƒëo√°n"
        
        df_features['daily_return'] = df_features['close'].pct_change().replace([np.inf, -np.inf], np.nan)
        df_features['high_low_range'] = ((df_features['high'] - df_features['low']) / df_features['close']).replace([np.inf, -np.inf], np.nan)
        
        if 'open' in df.columns:
            df_features['close_to_open'] = ((df_features['close'] - df_features['open']) / df_features['open']).replace([np.inf, -np.inf], np.nan)
        
        df_features['ma5'] = df_features['close'].rolling(window=5).mean()
        df_features['ma20'] = df_features['close'].rolling(window=20).mean()
        
        ma5 = df_features['ma5']
        ma20 = df_features['ma20']
        mask = (ma20 != 0) & ma20.notnull() & ma5.notnull()
        df_features['ma_ratio'] = np.nan
        df_features.loc[mask, 'ma_ratio'] = ma5.loc[mask] / ma20.loc[mask]
        
        df_features['volatility_5d'] = df_features['daily_return'].rolling(window=5).std().replace([np.inf, -np.inf], np.nan)
        

        df_features['volume_ma5'] = df_features['volume'].rolling(window=5).mean()
        

        vol = df_features['volume']
        vol_ma5 = df_features['volume_ma5']
        mask = (vol_ma5 != 0) & vol_ma5.notnull() & vol.notnull()
        df_features['volume_ratio'] = np.nan
        df_features.loc[mask, 'volume_ratio'] = vol.loc[mask] / vol_ma5.loc[mask]
        

        df_features = df_features.ffill()  
        df_features = df_features.fillna(0)  
        
        extended_features = features.copy()  
        
        additional_features = [
            'daily_return', 'high_low_range', 'ma_ratio', 
            'volatility_5d', 'volume_ratio'
        ]
        
        if 'open' in df.columns:
            additional_features.append('close_to_open')
            
        for feat in additional_features:
            if feat in df_features.columns:  
                if df_features[feat].isnull().any() or np.isinf(df_features[feat]).any():
                    df_features[feat] = df_features[feat].replace([np.inf, -np.inf], 0).fillna(0)
                extended_features.append(feat)
        

        df_check = df_features[extended_features]
        if df_check.isnull().any().any() or np.isinf(df_check).any().any():
            df_check = df_check.replace([np.inf, -np.inf], 0)
            df_check = df_check.fillna(0)
            df_features[extended_features] = df_check
        
        scaler = MinMaxScaler()
        try:
            scaled_data = scaler.fit_transform(df_features[extended_features])
        except Exception as scale_error:
            print(f"‚ùå {ticker} - L·ªói khi chu·∫©n h√≥a d·ªØ li·ªáu: {str(scale_error)}")
            # Hi·ªÉn th·ªã th√™m th√¥ng tin ƒë·ªÉ debug
            print(f"Ki·ªÉm tra gi√° tr·ªã kh√¥ng h·ª£p l·ªá: {df_features[extended_features].describe()}")
            return None

        def create_dataset(data, window_size=30):
            X, y = [], []
            for i in range(len(data) - window_size):
                X.append(data[i:i+window_size])
                y.append(data[i+window_size, 0])  
            return np.array(X), np.array(y)

        window_size = 30
        X, y = create_dataset(scaled_data, window_size)

        split = int(len(X) * 0.8)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]


        model = Sequential()
        model.add(LSTM(64, input_shape=(window_size, X.shape[2]), return_sequences=True))
        model.add(BatchNormalization())
        model.add(Dropout(0.2))
        model.add(LSTM(32))
        model.add(Dropout(0.2))
        model.add(Dense(16, activation='relu'))
        model.add(Dense(1))
        model.compile(optimizer='adam', loss='mean_squared_error')

        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
        model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2, 
                callbacks=[early_stop], verbose=0)
        

        y_pred = model.predict(X_test)
        
        y_pred_full = np.zeros((len(y_pred), len(extended_features)))
        y_pred_full[:, 0] = y_pred.flatten()  
        
        y_test_full = np.zeros((len(y_test), len(extended_features)))
        y_test_full[:, 0] = y_test  
        
        y_pred_inv = scaler.inverse_transform(y_pred_full)[:, 0]
        y_test_inv = scaler.inverse_transform(y_test_full)[:, 0]

        rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))
        
        r2 = r2_score(y_test_inv, y_pred_inv)
        
        start_idx = window_size + split
        actual_current_prices = df['close'].values[start_idx:start_idx+len(y_test_inv)-1]
        
        min_len = min(len(y_test_inv)-1, len(actual_current_prices))
        
        actual_direction = (y_test_inv[1:min_len+1] > actual_current_prices[:min_len]).astype(int)
        predicted_direction = (y_pred_inv[1:min_len+1] > actual_current_prices[:min_len]).astype(int)
        
        direction_accuracy = np.mean(actual_direction == predicted_direction)
        
        last_seq = scaled_data[-window_size:]
        last_seq = last_seq.reshape((1, window_size, len(extended_features)))
        next_day_scaled = model.predict(last_seq)
        
        next_day_full = np.zeros((1, len(extended_features)))
        next_day_full[0, 0] = next_day_scaled[0, 0]  
        
        next_day_price = scaler.inverse_transform(next_day_full)[0][0]
        
        current_price = df['close'].iloc[-1]
        change_pct = ((next_day_price - current_price) / current_price) * 100
        
        
        return {
            'ticker': ticker,
            'data_days': len(df),
            'train_days': len(y_train),
            'test_days': len(y_test),
            'rmse': rmse,
            'r2': r2,
            'direction_accuracy': direction_accuracy,
            'current_price': current_price,
            'predicted_price': next_day_price,
            'change_pct': change_pct,
            'trend': 'UP' if change_pct > 0 else 'DOWN'
        }
        
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω {ticker} v·ªõi LSTM: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def train_with_XGBoost_enhanced(ticker, num_days=300, test_size=0.2, val_size=0.15, tune_hyperparams=False):
    """
    Train m√¥ h√¨nh XGBoost v·ªõi s·ªë ng√†y d·ªØ li·ªáu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh, b·ªï sung nhi·ªÅu ch·ªâ b√°o k·ªπ thu·∫≠t v√† x·ª≠ l√Ω d·ªØ li·ªáu t·ªët h∆°n
    
    Args:
        ticker (str): M√£ c·ªï phi·∫øu
        num_days (int): S·ªë ng√†y d·ªØ li·ªáu mu·ªën s·ª≠ d·ª•ng
        test_size (float): T·ª∑ l·ªá d·ªØ li·ªáu test (0-1)
        val_size (float): T·ª∑ l·ªá d·ªØ li·ªáu validation (0-1)
        tune_hyperparams (bool): C√≥ th·ª±c hi·ªán t√¨m ki·∫øm hyperparameter t·ªëi ∆∞u kh√¥ng
        
    Returns:
        dict: K·∫øt qu·∫£ train v√† ƒë√°nh gi√° m√¥ h√¨nh
    """
    try:
        # Set up logging
        import logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger(f"XGBoost-{ticker}")
        
        logger.info(f"B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán XGBoost cho {ticker} v·ªõi {num_days} ng√†y d·ªØ li·ªáu")
        
        # 1. L·∫•y d·ªØ li·ªáu l·ªãch s·ª≠
        end_date = datetime.now()
        end_date_str = end_date.strftime('%Y-%m-%d')
        start_date = (end_date - timedelta(days=num_days*2)).strftime('%Y-%m-%d')  # L·∫•y th√™m d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω
        
        logger.info(f"ƒêang l·∫•y d·ªØ li·ªáu t·ª´ {start_date} ƒë·∫øn {end_date_str}")
        
        # Th·ª≠ c√°c ph∆∞∆°ng th·ª©c API kh√°c nhau c·ªßa vnstock
        try:
            # Ph∆∞∆°ng ph√°p 1: API m·ªõi
            stock = Vnstock().symbol(symbol=ticker)
            df = stock.quote.history(start=start_date, end=end_date_str, interval='1D')
            logger.info("L·∫•y d·ªØ li·ªáu th√†nh c√¥ng b·∫±ng API m·ªõi")
        except Exception as e1:
            logger.warning(f"API m·ªõi th·∫•t b·∫°i: {str(e1)}, th·ª≠ ph∆∞∆°ng ph√°p kh√°c...")
            try:
                # Ph∆∞∆°ng ph√°p 2: API c≈©
                from vnstock import stock_historical_data
                df = stock_historical_data(ticker, start_date, end_date_str, "1D")
                logger.info("L·∫•y d·ªØ li·ªáu th√†nh c√¥ng b·∫±ng API c≈©")
            except Exception as e2:
                logger.error(f"Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu: {str(e2)}")
                return None
        
        # 2. Ki·ªÉm tra v√† x·ª≠ l√Ω d·ªØ li·ªáu ban ƒë·∫ßu
        if df is None or df.empty or len(df) < 60:
            logger.error(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho {ticker}: ch·ªâ c√≥ {0 if df is None else len(df)} d√≤ng")
            return None
            
        # S·∫Øp x·∫øp theo th·ªùi gian n·∫øu c·∫ßn
        if 'time' in df.columns or 'date' in df.columns:
            date_col = 'time' if 'time' in df.columns else 'date'
            df[date_col] = pd.to_datetime(df[date_col])
            df = df.sort_values(by=date_col)
        
        # 3. X·ª≠ l√Ω ki·ªÉu d·ªØ li·ªáu v√† gi√° tr·ªã kh√¥ng h·ª£p l·ªá
        numeric_columns = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_columns:
            if col not in df.columns:
                logger.error(f"Thi·∫øu c·ªôt d·ªØ li·ªáu quan tr·ªçng: {col}")
                return None
                
            if df[col].dtype == 'object':
                try:
                    df[col] = df[col].astype(str).str.replace(',', '').astype(float)
                    logger.info(f"ƒê√£ chuy·ªÉn ƒë·ªïi {col} t·ª´ chu·ªói sang s·ªë")
                except Exception as e:
                    logger.error(f"L·ªói chuy·ªÉn ƒë·ªïi c·ªôt {col}: {str(e)}")
                    return None
        
        # Ki·ªÉm tra v√† x·ª≠ l√Ω gi√° tr·ªã kh√¥ng h·ª£p l·ªá
        for col in ['close', 'high', 'low', 'open']:
            # Ki·ªÉm tra gi√° √¢m ho·∫∑c b·∫±ng 0
            if (df[col] <= 0).any():
                bad_count = (df[col] <= 0).sum()
                logger.warning(f"Ph√°t hi·ªán {bad_count} gi√° tr·ªã kh√¥ng h·ª£p l·ªá trong {col}, ƒëang x·ª≠ l√Ω...")
                
                # Thay b·∫±ng gi√° tr·ªã h·ª£p l√Ω (gi√° trung b√¨nh c·ªßa d·ªØ li·ªáu h·ª£p l·ªá)
                valid_mean = df[df[col] > 0][col].mean()
                df.loc[df[col] <= 0, col] = valid_mean
        
        # X·ª≠ l√Ω volume b·∫•t th∆∞·ªùng
        if (df['volume'] <= 0).any():
            logger.warning(f"Ph√°t hi·ªán {(df['volume'] <= 0).sum()} gi√° tr·ªã volume kh√¥ng h·ª£p l·ªá, ƒëang x·ª≠ l√Ω...")
            valid_min_volume = max(1, df[df['volume'] > 0]['volume'].quantile(0.05))  # 5% percentile of valid volumes
            df.loc[df['volume'] <= 0, 'volume'] = valid_min_volume
                
        # 4. Feature Engineering: Th√™m c√°c ch·ªâ b√°o k·ªπ thu·∫≠t
        logger.info("B·∫Øt ƒë·∫ßu t·∫°o features...")
        
        # Volatility v√† Returns
        df['volatility'] = df['high'] - df['low']
        df['return_1d'] = df['close'].pct_change()
        df['log_return'] = np.log(df['close'] / df['close'].shift(1))
        
        # Moving Averages
        for window in [5, 10, 20, 50]:
            df[f'ma{window}'] = df['close'].rolling(window=window).mean()
            df[f'volume_ma{window}'] = df['volume'].rolling(window=window).mean()
        
        # Ratios
        df['ma5_ma20_ratio'] = df['ma5'] / df['ma20']
        df['close_ma20_ratio'] = df['close'] / df['ma20']
        
        # Volume Indicators
        df['volume_ratio'] = np.where(df['volume_ma5'] > 0, df['volume'] / df['volume_ma5'], 1)
        df['volume_change'] = df['volume'].pct_change()
        
        # Price change momentum
        for period in [1, 3, 5, 10]:
            df[f'price_momentum_{period}d'] = df['close'].pct_change(periods=period)
            
        # RSI (Relative Strength Index)
        delta = df['close'].diff()
        gain = delta.where(delta > 0, 0).rolling(window=14).mean()
        loss = -delta.where(delta < 0, 0).rolling(window=14).mean()
        rs = gain / loss.replace(0, 0.001)  # Tr√°nh chia cho 0
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # MACD (Moving Average Convergence Divergence)
        df['ema12'] = df['close'].ewm(span=12, adjust=False).mean()
        df['ema26'] = df['close'].ewm(span=26, adjust=False).mean()
        df['macd'] = df['ema12'] - df['ema26']
        df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']
        
        # Bollinger Bands
        df['bb_middle'] = df['close'].rolling(window=20).mean()
        df['bb_std'] = df['close'].rolling(window=20).std()
        df['bb_upper'] = df['bb_middle'] + (df['bb_std'] * 2)
        df['bb_lower'] = df['bb_middle'] - (df['bb_std'] * 2)
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        
        # ATR (Average True Range)
        df['tr1'] = df['high'] - df['low']
        df['tr2'] = abs(df['high'] - df['close'].shift(1))
        df['tr3'] = abs(df['low'] - df['close'].shift(1))
        df['true_range'] = df[['tr1', 'tr2', 'tr3']].max(axis=1)
        df['atr'] = df['true_range'].rolling(window=14).mean()
        
        # Target: Gi√° ƒë√≥ng c·ª≠a ng√†y ti·∫øp theo
        df['target'] = df['close'].shift(-1)
        
        # 5. X·ª≠ l√Ω NaN v√† v√¥ c·ª±c
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # X·ª≠ l√Ω NaN v·ªõi forward fill, sau ƒë√≥ backward fill
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        # N·∫øu v·∫´n c√≤n NaN, lo·∫°i b·ªè nh·ªØng d√≤ng ƒë√≥
        if df.isna().any().any():
            orig_len = len(df)
            df = df.dropna()
            logger.warning(f"ƒê√£ lo·∫°i b·ªè {orig_len - len(df)} d√≤ng c√≥ gi√° tr·ªã NaN")
        
        # 6. Ki·ªÉm tra s·ªë l∆∞·ª£ng d·ªØ li·ªáu c√≥ ƒë·ªß kh√¥ng
        total_days = len(df)
        if total_days < 60:  # Y√™u c·∫ßu t·ªëi thi·ªÉu
            logger.error(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu sau khi x·ª≠ l√Ω: ch·ªâ c√≤n {total_days} d√≤ng")
            return None
        
        # Gi·ªõi h·∫°n s·ªë ng√†y s·ª≠ d·ª•ng theo y√™u c·∫ßu
        if total_days > num_days:
            df = df.iloc[-num_days:].reset_index(drop=True)
            logger.info(f"Gi·ªõi h·∫°n d·ªØ li·ªáu xu·ªëng {num_days} ng√†y g·∫ßn nh·∫•t")
        
        # 7. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán
        # Ch·ªçn t·∫•t c·∫£ features tr·ª´ c√°c features kh√¥ng c·∫ßn thi·∫øt
        exclude_cols = ['target', 'time', 'date', 'tr1', 'tr2', 'tr3']
        features = [col for col in df.columns if col not in exclude_cols]
        
        logger.info(f"S·ª≠ d·ª•ng {len(features)} features: {', '.join(features[:10])}...")
        
        X = df[features]
        y = df['target']
        
        # 8. Ph√¢n chia d·ªØ li·ªáu th√†nh train, validation v√† test
        total_size = len(df)
        train_end = int(total_size * (1 - test_size - val_size))
        val_end = int(total_size * (1 - test_size))
        
        X_train, X_val, X_test = X[:train_end], X[train_end:val_end], X[val_end:]
        y_train, y_val, y_test = y[:train_end], y[train_end:val_end], y[val_end:]
        
        logger.info(f"Ph√¢n chia d·ªØ li·ªáu: Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)} m·∫´u")
        
        # 9. Hu·∫•n luy·ªán m√¥ h√¨nh XGBoost
        if tune_hyperparams:
            # T√¨m ki·∫øm hyperparameters t·ªët nh·∫•t
            logger.info("ƒêang th·ª±c hi·ªán t√¨m ki·∫øm hyperparameter...")
            
            from sklearn.model_selection import RandomizedSearchCV
            param_distributions = {
                'n_estimators': [100, 200, 300, 500],
                'max_depth': [2, 3, 4, 5, 6],
                'learning_rate': [0.01, 0.05, 0.1, 0.2],
                'subsample': [0.6, 0.7, 0.8, 0.9],
                'colsample_bytree': [0.6, 0.7, 0.8, 0.9],
                'gamma': [0, 0.1, 0.2],
                'min_child_weight': [1, 3, 5]
            }
            
            xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1)
            random_search = RandomizedSearchCV(
                xgb_model, param_distributions, n_iter=20, 
                scoring='neg_mean_squared_error', cv=3, verbose=0, random_state=42
            )
            
            random_search.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))
            best_params = random_search.best_params_
            logger.info(f"Hyperparameters t·ªët nh·∫•t: {best_params}")
            
            model = xgb.XGBRegressor(**best_params)
        else:
            # S·ª≠ d·ª•ng hyperparameters ƒë√£ ƒë∆∞·ª£c tinh ch·ªânh
            model = xgb.XGBRegressor(
                n_estimators=300, max_depth=3, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8, gamma=0.1, min_child_weight=3
            )
        
        # 10. Hu·∫•n luy·ªán v√† ƒë√°nh gi√°
        logger.info("ƒêang hu·∫•n luy·ªán m√¥ h√¨nh...")
        
        # T·∫°o t·∫≠p eval ƒë·ªÉ theo d√µi qu√° tr√¨nh hu·∫•n luy·ªán
        eval_set = [(X_train, y_train), (X_val, y_val)]
        
        model.fit(
            X_train, y_train, 
            eval_set=eval_set,
            eval_metric='rmse',
            early_stopping_rounds=20,
            verbose=False
        )
        
        # D·ª± ƒëo√°n tr√™n t·∫≠p test
        y_pred = model.predict(X_test)
        
        # L·∫•y gi√° hi·ªán t·∫°i v√† gi√° d·ª± ƒëo√°n ti·∫øp theo
        latest = df.iloc[-1:].copy()
        X_future = latest[features]
        predicted_price_tomorrow = model.predict(X_future)[0]
        current_price = df['close'].iloc[-1]
        change_pct = (predicted_price_tomorrow - current_price) / current_price * 100
        
        # 11. ƒê√°nh gi√° m√¥ h√¨nh
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        # T√≠nh to√°n direction accuracy
        y_direction_true = (y_test.values > X_test['close'].values).astype(int)
        y_direction_pred = (y_pred > X_test['close'].values).astype(int)
        direction_accuracy = np.mean(y_direction_true == y_direction_pred)
        
        # 12. Ph√¢n t√≠ch feature importance
        importance_df = pd.DataFrame({
            'feature': features,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        top_features = importance_df.head(10)
        logger.info(f"Top 10 features quan tr·ªçng nh·∫•t: {', '.join(top_features['feature'].tolist())}")
        
        # 13. K·∫øt qu·∫£ tr·∫£ v·ªÅ
        result = {
            'ticker': ticker,
            'data_days': len(df),
            'train_days': len(X_train),
            'val_days': len(X_val),
            'test_days': len(X_test),
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'direction_accuracy': direction_accuracy,
            'current_price': current_price,
            'predicted_price': predicted_price_tomorrow,
            'change_pct': change_pct,
            'trend': 'UP' if change_pct > 0 else 'DOWN',
            'top_features': top_features['feature'].tolist()[:5]
        }
        
        logger.info(f"K·∫øt qu·∫£ d·ª± ƒëo√°n cho {ticker}: RMSE={rmse:.4f}, R¬≤={r2:.4f}, Trend={'UP' if change_pct > 0 else 'DOWN'} ({change_pct:.2f}%)")
        return result
        
    except Exception as e:
        logger.error(f"L·ªói kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c khi hu·∫•n luy·ªán {ticker}: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def train_LSTM(ticker, test_size=0.2, val_size=0.1):
    """
    Train m√¥ h√¨nh LSTM cho d·ª± ƒëo√°n gi√° c·ªï phi·∫øu s·ª≠ d·ª•ng t·∫•t c·∫£ d·ªØ li·ªáu c√≥ s·∫µn
    
    Args:
        ticker (str): M√£ c·ªï phi·∫øu
        test_size (float): T·ª∑ l·ªá d·ªØ li·ªáu test
        val_size (float): T·ª∑ l·ªá d·ªØ li·ªáu validation
    Returns:
        dict: K·∫øt qu·∫£ train v√† ƒë√°nh gi√° m√¥ h√¨nh
    """
    try:
        # Set up logging
        import logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger(f"LSTM-{ticker}")
        
        logger.info(f"B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán LSTM cho {ticker} s·ª≠ d·ª•ng t·∫•t c·∫£ d·ªØ li·ªáu c√≥ s·∫µn")
        
        # 1. L·∫•y d·ªØ li·ªáu l·ªãch s·ª≠
        end_date = datetime.now()
        end_date_str = end_date.strftime('%Y-%m-%d')
        start_date = '2000-01-01'  # L·∫•y d·ªØ li·ªáu t·ª´ l√¢u ƒë·ªùi
        
        logger.info(f"ƒêang l·∫•y d·ªØ li·ªáu t·ª´ {start_date} ƒë·∫øn {end_date_str}")
        
        # Th·ª≠ c√°c ph∆∞∆°ng th·ª©c API kh√°c nhau
        try:
            # Ph∆∞∆°ng ph√°p 1: API m·ªõi
            stock = Vnstock().symbol(symbol=ticker)
            df = stock.quote.history(start=start_date, end=end_date_str, interval='1D')
            logger.info("L·∫•y d·ªØ li·ªáu th√†nh c√¥ng b·∫±ng API m·ªõi")
        except Exception as e1:
            logger.warning(f"API m·ªõi th·∫•t b·∫°i: {str(e1)}, th·ª≠ ph∆∞∆°ng ph√°p kh√°c...")
            try:
                # Ph∆∞∆°ng ph√°p 2: API c≈©
                from vnstock import stock_historical_data
                df = stock_historical_data(ticker, start_date, end_date_str, "1D")
                logger.info("L·∫•y d·ªØ li·ªáu th√†nh c√¥ng b·∫±ng API c≈©")
            except Exception as e2:
                logger.error(f"Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu: {str(e2)}")
                return None
        
        # 2. Ki·ªÉm tra d·ªØ li·ªáu
        if df is None or df.empty or len(df) < 60:
            logger.error(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho {ticker}: ch·ªâ c√≥ {0 if df is None else len(df)} d√≤ng")
            return None
            
        # S·∫Øp x·∫øp theo th·ªùi gian n·∫øu c·∫ßn
        if 'time' in df.columns or 'date' in df.columns:
            date_col = 'time' if 'time' in df.columns else 'date'
            df[date_col] = pd.to_datetime(df[date_col])
            df = df.sort_values(by=date_col)
        
        logger.info(f"ƒê√£ l·∫•y ƒë∆∞·ª£c {len(df)} ng√†y d·ªØ li·ªáu cho {ticker}")
        
        # 3. X√°c ƒë·ªãnh c√°c features c∆° b·∫£n
        if 'open' not in df.columns:
            features = ['close', 'volume', 'high', 'low']
        else:
            features = ['close', 'open', 'volume', 'high', 'low']
        
        # 4. Feature Engineering v√† X·ª≠ l√Ω d·ªØ li·ªáu
        df_features = df.copy()
        
        # X·ª≠ l√Ω volume b·∫•t th∆∞·ªùng
        if (df_features['volume'] <= 0).any():
            min_positive_volume = df_features.loc[df_features['volume'] > 0, 'volume'].min()
            df_features.loc[df_features['volume'] <= 0, 'volume'] = min_positive_volume * 0.01
            logger.warning(f"ƒê√£ ƒëi·ªÅu ch·ªânh {(df_features['volume'] <= 0).sum()} gi√° tr·ªã volume kh√¥ng h·ª£p l·ªá")
            
        # Ki·ªÉm tra gi√° √¢m ho·∫∑c b·∫±ng 0
        for price_col in ['close', 'high', 'low']:
            if (df_features[price_col] <= 0).any():
                logger.error(f"Ph√°t hi·ªán gi√° tr·ªã gi√° kh√¥ng h·ª£p l·ªá trong c·ªôt {price_col}")
                return None
        
        # T√≠nh to√°n c√°c ch·ªâ b√°o k·ªπ thu·∫≠t
        # Returns v√† Volatility
        df_features['daily_return'] = df_features['close'].pct_change().replace([np.inf, -np.inf], np.nan)
        df_features['high_low_range'] = ((df_features['high'] - df_features['low']) / df_features['close']).replace([np.inf, -np.inf], np.nan)
        
        if 'open' in df.columns:
            df_features['close_to_open'] = ((df_features['close'] - df_features['open']) / df_features['open']).replace([np.inf, -np.inf], np.nan)
        
        # Moving Averages
        df_features['ma5'] = df_features['close'].rolling(window=5).mean()
        df_features['ma20'] = df_features['close'].rolling(window=20).mean()
        
        # Moving Average Ratio
        ma5 = df_features['ma5']
        ma20 = df_features['ma20']
        mask = (ma20 != 0) & ma20.notnull() & ma5.notnull()
        df_features['ma_ratio'] = np.nan
        df_features.loc[mask, 'ma_ratio'] = ma5.loc[mask] / ma20.loc[mask]
        
        # Volatility
        df_features['volatility_5d'] = df_features['daily_return'].rolling(window=5).std().replace([np.inf, -np.inf], np.nan)
        
        # Volume Indicators
        df_features['volume_ma5'] = df_features['volume'].rolling(window=5).mean()
        
        # Volume Ratio
        vol = df_features['volume']
        vol_ma5 = df_features['volume_ma5']
        mask = (vol_ma5 != 0) & vol_ma5.notnull() & vol.notnull()
        df_features['volume_ratio'] = np.nan
        df_features.loc[mask, 'volume_ratio'] = vol.loc[mask] / vol_ma5.loc[mask]
        
        # RSI - Relative Strength Index
        delta = df_features['close'].diff()
        gain = delta.where(delta > 0, 0).rolling(window=14).mean()
        loss = -delta.where(delta < 0, 0).rolling(window=14).mean()
        rs = gain / loss.replace(0, 0.001)  # Tr√°nh chia cho 0
        df_features['rsi'] = 100 - (100 / (1 + rs))
        
        # MACD - Moving Average Convergence Divergence
        df_features['ema12'] = df_features['close'].ewm(span=12, adjust=False).mean()
        df_features['ema26'] = df_features['close'].ewm(span=26, adjust=False).mean()
        df_features['macd'] = df_features['ema12'] - df_features['ema26']
        df_features['macd_signal'] = df_features['macd'].ewm(span=9, adjust=False).mean()
        
        # X·ª≠ l√Ω missing values
        df_features = df_features.ffill()  # forward fill
        df_features = df_features.fillna(0)  # c√°c gi√° tr·ªã NaN c√≤n l·∫°i
        
        # 5. K·∫øt h·ª£p c√°c features
        extended_features = features.copy()  
        
        # C√°c features b·ªï sung
        additional_features = [
            'daily_return', 'high_low_range', 'ma_ratio', 
            'volatility_5d', 'volume_ratio', 'rsi', 'macd', 'macd_signal'
        ]
        
        if 'open' in df.columns:
            additional_features.append('close_to_open')
        
        # Ki·ªÉm tra v√† th√™m features
        for feat in additional_features:
            if feat in df_features.columns:  
                if df_features[feat].isnull().any() or np.isinf(df_features[feat]).any():
                    df_features[feat] = df_features[feat].replace([np.inf, -np.inf], 0).fillna(0)
                extended_features.append(feat)
        
        # Ki·ªÉm tra cu·ªëi c√πng c√°c gi√° tr·ªã kh√¥ng h·ª£p l·ªá
        df_check = df_features[extended_features]
        if df_check.isnull().any().any() or np.isinf(df_check).any().any():
            df_check = df_check.replace([np.inf, -np.inf], 0)
            df_check = df_check.fillna(0)
            df_features[extended_features] = df_check
        
        # 6. Chu·∫©n h√≥a d·ªØ li·ªáu
        scaler = MinMaxScaler()
        try:
            scaled_data = scaler.fit_transform(df_features[extended_features])
        except Exception as scale_error:
            logger.error(f"L·ªói khi chu·∫©n h√≥a d·ªØ li·ªáu: {str(scale_error)}")
            return None

        # 7. T·∫°o d·ªØ li·ªáu chu·ªói th·ªùi gian
        window_size = 30  # Gi·ªØ nguy√™n window_size = 30 nh∆∞ trong code g·ªëc
        
        def create_dataset(data, window_size=30):
            X, y = [], []
            for i in range(len(data) - window_size):
                X.append(data[i:i+window_size])
                y.append(data[i+window_size, 0])  # D·ª± ƒëo√°n gi√° ƒë√≥ng c·ª≠a
            return np.array(X), np.array(y)

        X, y = create_dataset(scaled_data, window_size)
        
        # Ki·ªÉm tra ƒë·ªß d·ªØ li·ªáu cho train/val/test
        if len(X) < 60:  # y√™u c·∫ßu t·ªëi thi·ªÉu
            logger.error(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu sau khi t·∫°o chu·ªói: ch·ªâ c√≥ {len(X)} m·∫´u")
            return None

        # 8. Ph√¢n chia train/validation/test
        train_size = int(len(X) * (1 - test_size - val_size))
        val_size_samples = int(len(X) * val_size)
        
        X_train = X[:train_size]
        y_train = y[:train_size]
        
        X_val = X[train_size:train_size+val_size_samples]
        y_val = y[train_size:train_size+val_size_samples]
        
        X_test = X[train_size+val_size_samples:]
        y_test = y[train_size+val_size_samples:]
        
        logger.info(f"Ph√¢n chia d·ªØ li·ªáu: Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)} m·∫´u")

        # 9. X√¢y d·ª±ng v√† hu·∫•n luy·ªán m√¥ h√¨nh
        from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
        from tensorflow.keras.layers import Dropout, BatchNormalization
        
        # Ki·∫øn tr√∫c n√¢ng cao v·ªõi dropout v√† batch normalization
        model = Sequential()
        model.add(LSTM(64, input_shape=(window_size, X.shape[2]), return_sequences=True))
        model.add(BatchNormalization())
        model.add(Dropout(0.2))
        model.add(LSTM(32))
        model.add(Dropout(0.2))
        model.add(Dense(1))
        
        model.compile(optimizer='adam', loss='mean_squared_error')
        
        # Callbacks
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)
        ]
        
        # Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi validation data
        logger.info("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh LSTM")
        model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=50,  # gi·ªØ nguy√™n s·ªë epochs=50 nh∆∞ code g·ªëc
            batch_size=16,
            callbacks=callbacks,
            verbose=0
        )
        
        # 10. ƒê√°nh gi√° m√¥ h√¨nh
        logger.info("ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test")
        y_pred = model.predict(X_test)
        
        # Chuy·ªÉn ƒë·ªïi v·ªÅ thang ƒëo g·ªëc
        y_pred_full = np.zeros((len(y_pred), len(extended_features)))
        y_pred_full[:, 0] = y_pred.flatten()
        
        y_test_full = np.zeros((len(y_test), len(extended_features)))
        y_test_full[:, 0] = y_test
        
        y_pred_inv = scaler.inverse_transform(y_pred_full)[:, 0]
        y_test_inv = scaler.inverse_transform(y_test_full)[:, 0]

        # T√≠nh c√°c metrics
        rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))
        r2 = r2_score(y_test_inv, y_pred_inv)
        mae = mean_absolute_error(y_test_inv, y_pred_inv)
        
        # T√≠nh direction accuracy
        start_idx = window_size + train_size + val_size_samples
        actual_current_prices = df['close'].values[start_idx:start_idx+len(y_test_inv)-1]
        
        min_len = min(len(y_test_inv)-1, len(actual_current_prices))
        
        actual_direction = (y_test_inv[1:min_len+1] > actual_current_prices[:min_len]).astype(int)
        predicted_direction = (y_pred_inv[1:min_len+1] > actual_current_prices[:min_len]).astype(int)
        
        direction_accuracy = np.mean(actual_direction == predicted_direction)
        
        # 11. D·ª± ƒëo√°n gi√° cho ng√†y ti·∫øp theo
        last_seq = scaled_data[-window_size:]
        last_seq = last_seq.reshape((1, window_size, len(extended_features)))
        next_day_scaled = model.predict(last_seq)
        
        next_day_full = np.zeros((1, len(extended_features)))
        next_day_full[0, 0] = next_day_scaled[0, 0]
        
        next_day_price = scaler.inverse_transform(next_day_full)[0][0]
        
        current_price = df['close'].iloc[-1]
        change_pct = ((next_day_price - current_price) / current_price) * 100
        
        # 12. K·∫øt qu·∫£ tr·∫£ v·ªÅ
        result = {
            'ticker': ticker,
            'data_days': len(df),
            'train_days': len(X_train),
            'val_days': len(X_val),
            'test_days': len(X_test),
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'direction_accuracy': direction_accuracy,
            'current_price': current_price,
            'predicted_price': next_day_price,
            'change_pct': change_pct,
            'trend': 'UP' if change_pct > 0 else 'DOWN'
        }
        
        logger.info(f"K·∫øt qu·∫£ d·ª± ƒëo√°n cho {ticker}: RMSE={rmse:.4f}, R¬≤={r2:.4f}, Trend={'UP' if change_pct > 0 else 'DOWN'} ({change_pct:.2f}%)")
        return result
        
    except Exception as e:
        logging.error(f"‚ùå L·ªói x·ª≠ l√Ω {ticker} v·ªõi LSTM: {str(e)}")
        import traceback
        traceback.print_exc()
        return None
